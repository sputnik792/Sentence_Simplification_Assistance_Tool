{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "All Pretrained models needed for this project are already  via pip install, no need for any external connections"
      ],
      "metadata": {
        "id": "gwbUPeXgoYEF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXYPNo5L5GgQ"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install flair\n",
        "!pip install PyDictionary\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language-tool-python\n",
        "!pip install wordfreq\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_md\n",
        "# !python -m spacy download en_core_web_lg\n",
        "!pip install transformers sentencepiece sacremoses"
      ],
      "metadata": {
        "id": "SaXpZzqt9VWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWjjGWy34_PQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import *\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "import spacy\n",
        "import flair\n",
        "from wordfreq import zipf_frequency\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nlp_small = spacy.load(\"en_core_web_sm\")\n",
        "nlp_medium = spacy.load(\"en_core_web_md\")\n",
        "# nlp_large = spacy.load(\"en_core_web_lg\")\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "import string\n",
        "Dataset = namedtuple('Dataset', 'name, train, test')\n",
        "Model = namedtuple('Model', 'type, name, dimension, corpus, model')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/complex-word-identification-dataset/cwishareddataset.zip\n",
        "# !unzip cwishareddataset.zip"
      ],
      "metadata": {
        "id": "werJbY-CH13o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.set_option('display.max_columns', 500)\n",
        "# pd.set_option('display.max_colwidth', 200)\n",
        "# MAIN_PATH_DATASET = \"traindevset/english/\"\n",
        "# genres = ['Wikipedia', 'WikiNews', 'News']\n",
        "# datasets = ['Train', 'Dev']\n",
        "# columns = ['id', 'sentence', \"start\", \"end\", \"target\", \n",
        "#            \"nat\", \"non_nat\", \"nat_marked\", \"non_nat_marked\", \"binary\", \"prob\"]\n",
        "\n",
        "\n",
        "# datasets = [Dataset('Wikipedia', 'Train', 'Dev'),\n",
        "#             Dataset('WikiNews', 'Train', 'Dev'),\n",
        "#             Dataset('News', 'Train', 'Dev')]\n",
        "\n",
        "# feature_categories = []\n",
        "\n",
        "# def load_df(path):\n",
        "#     df = pd.read_csv(path, header=None, sep = \"\\t\")\n",
        "#     df.columns = columns\n",
        "#     return df\n",
        "\n",
        "# datasets = [Dataset(d.name, load_df(MAIN_PATH_DATASET + d.name + '_' + d.train + '.tsv'),\n",
        "#                             load_df(MAIN_PATH_DATASET + d.name + '_' + d.test + '.tsv'))\n",
        "#                             for d in datasets]"
      ],
      "metadata": {
        "id": "nAglpCmaKn6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy"
      ],
      "metadata": {
        "id": "4nw23InTfceu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ9Sbbvx3O1_"
      },
      "outputs": [],
      "source": [
        "model_ST_bert = SentenceTransformer('bert-base-nli-mean-tokens')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q5FgvvT3Wtp"
      },
      "outputs": [],
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "# models we gonna use for this project\n",
        "\n",
        "model_ctrl44 = BartForConditionalGeneration.from_pretrained(\"liamcripwell/ctrl44-simp\")\n",
        "tokenizer_ctrl44 = AutoTokenizer.from_pretrained(\"liamcripwell/ctrl44-simp\")\n",
        "\n",
        "model_pega = PegasusForConditionalGeneration.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
        "tokenizer_pega = PegasusTokenizerFast.from_pretrained(\"tuner007/pegasus_paraphrase\")\n",
        "\n",
        "tokenizer_paws = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
        "model_paws = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
        "\n",
        "tokenizer_parr = AutoTokenizer.from_pretrained(\"prithivida/parrot_paraphraser_on_T5\")\n",
        "model_parr = AutoModelForSeq2SeqLM.from_pretrained(\"prithivida/parrot_paraphraser_on_T5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFNWRIMc3Wvq"
      },
      "outputs": [],
      "source": [
        "def get_paraphrased_sentences(model, tokenizer, sentence, num_return_sequences=5, num_beams=5, max_length=128):\n",
        "  # tokenize the text to be form of a list of token IDs\n",
        "  inputs = tokenizer([sentence], truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "  # generate the paraphrased sentences\n",
        "  outputs = model.generate(\n",
        "    **inputs,\n",
        "    num_beams=num_beams,\n",
        "    num_return_sequences=num_return_sequences,\n",
        "    max_length=max_length\n",
        "  )\n",
        "  # decode the generated sentences using the tokenizer to get them back to text\n",
        "  return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_words(word_used, tag):\n",
        "  zip_scores_dict = {}\n",
        "  sim_scores_dict = {}\n",
        "  syn_list = extract_synonyms(word_used, tag)  # SynSets nltk\n",
        "  # hyper_list = extract_hypernyms(word_used, tag) \n",
        "  for syn in syn_list:\n",
        "    print(syn)\n",
        "    tokens = nlp_medium(word_used + \" \" + syn)\n",
        "    token1 = tokens[0]\n",
        "    token2 = tokens[1]\n",
        "    sim_score = token1.similarity(token2)\n",
        "    if syn != word_used: \n",
        "      zip_scores_dict[syn] = zipf_frequency(syn, 'en')\n",
        "      sim_scores_dict[syn] = sim_score\n",
        "      print(syn, \": zipf_score= \", zipf_frequency(syn, 'en'), \", sim_score= \", sim_score)\n",
        "  \n",
        "  sim_scores_dict = sort_dict(sim_scores_dict)\n",
        "  zip_scores_dict = sort_dict(zip_scores_dict)\n",
        "\n",
        "  highest_group_words = []\n",
        "  ind = len(sim_scores_dict) - 1\n",
        "  print(ind)\n",
        "  best_sim_score = sim_scores_dict[list(sim_scores_dict.keys())[ind]]\n",
        "  highest_group_words.append(list(sim_scores_dict.keys())[ind])\n",
        "  flag = 1\n",
        "  while flag == 1 and ind >= 0:\n",
        "    ind = ind - 1\n",
        "    if sim_scores_dict[list(sim_scores_dict.keys())[ind]] == best_sim_score:\n",
        "      highest_group_words.append(list(sim_scores_dict.keys())[ind])\n",
        "    else:\n",
        "      flag = 0\n",
        "\n",
        "  if len(highest_group_words) == 1:\n",
        "    return highest_group_words[0]\n",
        "  else:\n",
        "    flag = 1\n",
        "    ind = len(zip_scores_dict)\n",
        "    while flag == 1 and ind >= 0:\n",
        "      ind = ind - 1\n",
        "      if list(sim_scores_dict.keys())[ind] in highest_group_words:\n",
        "        return list(sim_scores_dict.keys())[ind]\n",
        "    return list(zip_scores_dict.keys())[-1]\n",
        "\n",
        "  #cos_sims_bert = get_cos_similarities(syn_list, word_used)\n",
        "\n"
      ],
      "metadata": {
        "id": "t0fmmbmBZAu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUgoMQ0q7tlM"
      },
      "outputs": [],
      "source": [
        "## Using NLTK SynSets\n",
        "\n",
        "def extract_synonyms(word, p):\n",
        "  synonyms = []\n",
        "\n",
        "  # part of speech\n",
        "  if (p == \"verb\"):\n",
        "    syns = wordnet.synsets(word, pos = wordnet.VERB)\n",
        "  if (p == \"noun\"):\n",
        "    syns = wordnet.synsets(word, pos = wordnet.NOUN)\n",
        "  if (p == \"adj\"):\n",
        "    syns = wordnet.synsets(word, pos = wordnet.ADJ)\n",
        "    \n",
        "  for syn in syns:\n",
        "    for lem in syn.lemmas():\n",
        "      synonyms.append(lem.name())\n",
        "  \n",
        "  return(list(set(synonyms)))\n",
        "\n",
        "def extract_hypernyms(word, p):\n",
        "\thypernyms = []\n",
        "\n",
        "\t# part of speech\n",
        "\tif (p == \"verb\"):\n",
        "\t\tsyns2 = wordnet.synsets(word, pos = wordnet.VERB)\n",
        "\tif (p == \"noun\"):\n",
        "\t\tsyns2 = wordnet.synsets(word, pos = wordnet.NOUN)\n",
        "\tif (p == \"adj\"):\n",
        "\t\tsyns2 = wordnet.synsets(word, pos = wordnet.ADJ)\n",
        "\t\n",
        "\n",
        "\tfor syn in syns2:\n",
        "\t\tfor hyper in syn.hypernyms():\n",
        "\t\t\thypernyms.append(hyper)\n",
        "\t\n",
        "\treturn(list(set(hypernyms)))\n",
        "\n",
        "def extract_hyponyms(word, p):\n",
        "  hyponyms = []\n",
        "\n",
        "  # part of speech\n",
        "  if (p == \"verb\"):\n",
        "    syns2 = wordnet.synsets(word, pos = wordnet.VERB)\n",
        "  if (p == \"noun\"):\n",
        "    syns2 = wordnet.synsets(word, pos = wordnet.NOUN)\n",
        "  if (p == \"adj\"):\n",
        "    syns2 = wordnet.synsets(word, pos = wordnet.ADJ)\n",
        "    \n",
        "  for syn in syns2:\n",
        "    for hypo in syn.hyponyms():\n",
        "      hyponyms.append(hypo)\n",
        "  \n",
        "  return(list(set(hyponyms)))\n",
        "\n",
        "def shortest_hypernym_distances(word, p):\n",
        "  distances = []\n",
        "  shortest = 99999999\n",
        "  result = []\n",
        "  \n",
        "  # part of speech\n",
        "  if (p == \"verb\"):\n",
        "    syns3 = wordnet.synsets(word, pos = wordnet.VERB)\n",
        "  if (p == \"noun\"):\n",
        "    syns3 = wordnet.synsets(word, pos = wordnet.NOUN)\n",
        "  if (p == \"adj\"):\n",
        "    syns3 = wordnet.synsets(word, pos = wordnet.ADJ)\n",
        "\n",
        "  for syn in syns3:\n",
        "    for path in syn.hypernym_paths():\n",
        "      if len(path) < shortest:\n",
        "        shortest = min([len(path) for path in syn.hypernym_paths()])\n",
        "        result = [path]\n",
        "    distances.append(min([len(path) for path in syn.hypernym_paths()]))\n",
        "  \n",
        "  if(result):\n",
        "    return result;\n",
        "  print(\"ERROR - no shortest hypernym distances.\");\n",
        "\n",
        "#display(shortest_hypernym_distances(\"love\", \"noun\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_POS_groups = [\"noun\", \"verb\", \"adj\"]\n",
        "SUBJECT_PRONOUNS = [\"I\", \"you\", \"he\", \"she\", \"we\", \"they\"]\n",
        "OBJECT_PRONOUNS = [\"me\", \"you\", \"him\", \"her\", \"us\", \"them\", \"whom\"]\n",
        "POSS_PRONOUNS = [\"mine\", \"ours\", \"yours\", \"his\", \"hers\", \"its\", \"theirs\"]\n",
        "POSS_DETS = [\"my\", \"our\", \"your\", \"his\", \"her\", \"its\", \"their\"]\n",
        "# special rule to keep determiners for nouns\n",
        "MAIN_DETS = [\"a\", \"an\", \"the\", \"some\"]\n",
        "MAIN_POS_TAGS = {\"JJ\":\t\"adj\", \"JJR\": \"adj\", \"JJS\": \"adj\", \"NNP\":\t\"propernoun\", \n",
        "            \"NNPS\": \"propernoun\", \"RB\": \"adverb\", \"RBR\": \"adverb\", \"RBS\": \"adverb\",\n",
        "            \"VB\": \"verb\", \"VBD\": \"verb\", \"VBG\": \"verb\", \"VBN\": \"verb\", \"VBP\": \"verb\", \"VBZ\": \"verb\",\n",
        "            \"WRB\": \"adverb\", \"NN\": 'noun', 'NNS': 'noun'\n",
        "# 33.\tWDT\tWh-determiner\n",
        "# 34.\tWP\tWh-pronoun\n",
        "# 35.\tWP$\tPossessive wh-pronoun\n",
        "# 36.\tWRB\tWh-adverb\n",
        "}"
      ],
      "metadata": {
        "id": "TR6fx6kN_rbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkN_KFa5AxWZ"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"Three years later, the coffin was still full of Jello.\",\n",
        "    \"The fish contemplated escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
        "    \"The person box was packed with jelly many dozens of months later.\",\n",
        "    \"The way the building was designed seemed counterintuitive to most onlookers.\",\n",
        "    \"They found many egregious mistakes in his calculations.\",\n",
        "    \"He found a gargantuan walnut in his flask.\",\n",
        "    \"There aren't any more bananas\",\n",
        "    \"David and his prolixity\"\n",
        "]\n",
        "\n",
        "def get_pos_tags_list(s):\n",
        "  pos_sentences = []\n",
        "  text = word_tokenize(s)\n",
        "  pos_tags = nltk.pos_tag(text)\n",
        "  pos_sentences.append(pos_tags)\n",
        "  return pos_sentences\n",
        "\n",
        "for s in sentences:\n",
        "  print(get_pos_tags_list(s))\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import language_tool_python\n",
        "pylangTool = language_tool_python.LanguageTool('en-US')\n",
        "from PyDictionary import PyDictionary\n",
        "dictionary=PyDictionary(\"hotel\",\"ambush\",\"nonchalant\",\"perceptive\")\n",
        "print(dictionary.printMeanings()) \n",
        "print(dictionary.getMeanings()) \n",
        "\n",
        "words = set(nlp_small.vocab.strings)\n",
        "word = 'would'\n",
        "print(f\"Is '{word}' an English word: {word in words}\") "
      ],
      "metadata": {
        "id": "vbSWhjQv7oEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary=PyDictionary(\"hotel\",\"ambush\",\"nonchalant\",\"perceptive\")\n",
        "print(dictionary.printMeanings()) \n",
        "print(dictionary.getMeanings()) \n",
        "\n",
        "dictionary=PyDictionary(\"blithe\")\n",
        "print(dictionary.printMeanings()) \n",
        "print(dictionary.getMeanings()) "
      ],
      "metadata": {
        "id": "rHO17uiZsjjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Your the best but their are allso  good !\"\n",
        "matches = pylangTool.check(text)\n",
        "len(matches)\n",
        "\n",
        "print(matches[0])\n",
        "\n",
        "\n",
        "text = \"appeared disappear\"\n",
        "matches = pylangTool.check(text)\n",
        "len(matches)\n",
        "print(matches)\n",
        "\n"
      ],
      "metadata": {
        "id": "IdoKs8MD9-hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"His natural reticence seemed under the influence of alcohol to disappear\"\n",
        "matches = pylangTool.check(text)\n",
        "print(\"num of errors: \", len(matches))\n",
        "\n",
        "dd = pylangTool.correct(\"His natural reticence seemed under the influence of alcohol to disappear\")\n",
        "print(dd)"
      ],
      "metadata": {
        "id": "br1n02e7G093"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nlp_medium(\"huge gigantic enormous big\")\n",
        "  \n",
        "for token in tokens:\n",
        "    # Printing the following attributes of each token.\n",
        "    # text: the word string, has_vector: if it contains\n",
        "    # a vector representation in the model, \n",
        "    # vector_norm: the algebraic norm of the vector,\n",
        "    # is_oov: if the word is out of vocabulary.\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
        "  \n",
        "\n",
        "for t1 in range(0, len(tokens)):\n",
        "  token1 = tokens[t1]\n",
        "  for t2 in range(t1, len(tokens)):\n",
        "    token2 = tokens[t2]\n",
        "    print(\"Similarity: {}, {}\".format(token1, token2), token1.similarity(token2))\n"
      ],
      "metadata": {
        "id": "p8lplIvmq124"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "\n",
        "def get_important_word_stats(sent, z_freqs_Test):\n",
        "  arr = list(z_freqs_Test.values())\n",
        "  print('median=', np.median(arr))\n",
        "  npquart1 = np.quantile(arr, 0.25)\n",
        "  print('1st quartile=', npquart1)\n",
        "\n",
        "  z_freqs_Test = sort_dict(z_freqs_Test)\n",
        "  zkeys = []\n",
        "  zvalues = []\n",
        "  for k,v in z_freqs_Test.items():\n",
        "    zkeys.append(k)\n",
        "    zvalues.append(v)\n",
        "\n",
        "  ### hard_constraint = requires sentence to retain more context words\n",
        "  hard_constraint_set = zkeys[0: len(zkeys)//2]\n",
        "  soft_cut = -1\n",
        "  for vv in zvalues:\n",
        "    if vv <= npquart1:  ## below the 1st quartile\n",
        "      soft_cut = soft_cut + 1\n",
        "\n",
        "  soft_constraint_set = zkeys[0: soft_cut]\n",
        "  return hard_constraint_set, soft_constraint_set"
      ],
      "metadata": {
        "id": "FGjeGk0Ts_X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"David likes eating peanut butter with a spoon rather than a fork\"\n",
        "\n",
        "print(\" \")\n",
        "\n",
        "set_1 = set(get_paraphrased_sentences(model_pega, tokenizer_pega, test_sentence, num_beams=20, num_return_sequences=20))\n",
        "set_2 = set(get_paraphrased_sentences(model_paws, tokenizer_paws, test_sentence, num_beams=20, num_return_sequences=20))\n",
        "set_3 = set(get_paraphrased_sentences(model_parr, tokenizer_parr, test_sentence, num_beams=20, num_return_sequences=20))\n",
        "\n",
        "combined_set = set_1.union(set_2).union(set_3) \n",
        "for x in combined_set:\n",
        "  print(x)"
      ],
      "metadata": {
        "id": "r3NxA5nSILRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def easy_simplify(test_sentence):\n",
        "  set_1 = set(get_paraphrased_sentences(model_pega, tokenizer_pega, test_sentence, num_beams=20, num_return_sequences=20))\n",
        "  set_2 = set(get_paraphrased_sentences(model_paws, tokenizer_paws, test_sentence, num_beams=20, num_return_sequences=20))\n",
        "  set_3 = set(get_paraphrased_sentences(model_parr, tokenizer_parr, test_sentence, num_beams=20, num_return_sequences=20))\n",
        "  combined_set = set_1.union(set_2).union(set_3) \n",
        "  return combined_set\n",
        "\n",
        "# combined_set_ex = easy_simplify(\"Dave's long-windedness was slowing down the meeting\")\n",
        "# for x in combined_set_ex:\n",
        "#   print(x)"
      ],
      "metadata": {
        "id": "er-sM3-aBkuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "snow_stemmer = SnowballStemmer(\"english\")\n",
        "from PyDictionary import PyDictionary\n",
        "dictionary=PyDictionary(\"prolixity\")\n",
        "print(dictionary.printMeanings()) \n",
        "print(dictionary.getMeanings()) \n",
        "\n",
        "nlp_medium_words = set(nlp_medium.vocab.strings)\n",
        "word_used = 'prolixity'\n",
        "print(f\"Is '{word_used}' an English word: {word_used in nlp_medium_words}\") \n",
        "\n",
        "print(snow_stemmer.stem(\"generalized\"))\n",
        "print(snow_stemmer.stem(\"generalization\"))\n",
        "print(snow_stemmer.stem(\"hating\"))\n",
        "\n",
        "port_stemmer = PorterStemmer()\n",
        "  \n",
        "# choose some words to be stemmed\n",
        "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\", \"hating\"]\n",
        "  \n",
        "for w in words:\n",
        "    print(w, \" : \", port_stemmer.stem(w))\n",
        "\n"
      ],
      "metadata": {
        "id": "fBc4MGAaJ2FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The real project to be used start here:"
      ],
      "metadata": {
        "id": "LgzwBwaNKkfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punct(sent):\n",
        "  test_str = sent.translate(str.maketrans('', '', string.punctuation))\n",
        "  st_arr = test_str.split()\n",
        "  curr_sent_no_punct = st_arr[0]\n",
        "  for i in range(1,len(st_arr)):\n",
        "    curr_sent_no_punct = curr_sent_no_punct + \" \" + st_arr[i]\n",
        "  return curr_sent_no_punct"
      ],
      "metadata": {
        "id": "oHywmApWwvbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "def get_word_counts(sent):\n",
        "  words = remove_punct(sent).lower().split()\n",
        "  word_counts = collections.Counter(words)\n",
        "  return word_counts"
      ],
      "metadata": {
        "id": "5ovAWU7gzNfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('cmudict')\n",
        "from nltk.corpus import cmudict\n",
        "cmudic = cmudict.dict()\n",
        "\n",
        "def nsyllables(word):\n",
        "    try:\n",
        "        h = [len(list(y for y in x if y[-1].isdigit())) for x in cmudic[word.lower()]]\n",
        "        return h[0]\n",
        "    except KeyError:\n",
        "        #if word not found in cmudict\n",
        "        return \n",
        "\n",
        "def syllables(word):\n",
        "    count = 0\n",
        "    vowels = 'aeiouy'\n",
        "    word = word.lower()\n",
        "    if word[0] in vowels:\n",
        "        count +=1\n",
        "    for index in range(1,len(word)):\n",
        "        if word[index] in vowels and word[index-1] not in vowels:\n",
        "            count +=1\n",
        "    if word.endswith('e'):\n",
        "        count -= 1\n",
        "    if word.endswith('le'):\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count"
      ],
      "metadata": {
        "id": "Bim8eAIL_BH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def break_sentences(text):\n",
        "    doc = nlp_small(text)\n",
        "    return list(doc.sents)\n",
        " \n",
        "def word_count(text):\n",
        "    sentences = break_sentences(text)\n",
        "    words = 0\n",
        "    for sentence in sentences:\n",
        "        words += len([token for token in sentence])\n",
        "    return words\n",
        " \n",
        "def sentence_count(text):\n",
        "    sentences = break_sentences(text)\n",
        "    return len(sentences)\n",
        " \n",
        "def avg_sentence_length(text):\n",
        "    words = word_count(text)\n",
        "    sentences = sentence_count(text)\n",
        "    average_sentence_length = float(words / sentences)\n",
        "    return average_sentence_length\n",
        " \n",
        "def syllables_count(word):\n",
        "    return nsyllables(word)\n",
        " \n",
        "# Return total Difficult Words in a text\n",
        "def difficult_words(text):\n",
        "     \n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    # Find all words in the text\n",
        "    words = []\n",
        "    sentences = break_sentences(text)\n",
        "    for sentence in sentences:\n",
        "        words += [str(token) for token in sentence]\n",
        " \n",
        "    # difficult words are those with syllables >= 2\n",
        "    # easy_word_set is provide by Textstat as\n",
        "    # a list of common words\n",
        "    diff_words_set = set()\n",
        "     \n",
        "    for word in words:\n",
        "        syllable_count = syllables_count(word)\n",
        "        if word not in stop_words_nltk and syllable_count >= 2:\n",
        "            diff_words_set.add(word)\n",
        " \n",
        "    return len(diff_words_set)\n",
        " \n",
        "# A word is polysyllablic if it has more than 3 syllables\n",
        "# this functions returns the number of all such words\n",
        "# present in the text\n",
        "def poly_syllable_count(text):\n",
        "    count = 0\n",
        "    words = []\n",
        "    sentences = break_sentences(text)\n",
        "    for sentence in sentences:\n",
        "        words += [token for token in sentence]\n",
        "    for word in words:\n",
        "        syllable_count = syllables_count(word)\n",
        "        if syllable_count >= 3:\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "def gunning_fog(text):\n",
        "    per_diff_words = (difficult_words(text) / word_count(text) * 100) + 5\n",
        "    grade = 0.4 * (avg_sentence_length(text) + per_diff_words)\n",
        "    return grade\n",
        " \n",
        " "
      ],
      "metadata": {
        "id": "n3vSFbAgHwG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concat_all_words(arr):\n",
        "  ans = \"\"\n",
        "  for i in arr:\n",
        "    # concatenating the strings\n",
        "    # using + operator\n",
        "    ans = ans + \" \" + i\n",
        "  return ans\n",
        "\n",
        "\n",
        "# sorting by the value\n",
        "def sort_dict(d):\n",
        "  return {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}"
      ],
      "metadata": {
        "id": "GWP986Ma8FJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## asa_set can be a set of words or set of sentences, same for original_sentence\n",
        "def get_cos_similarities(asa_set, original_sentence):\n",
        "  asa_set.insert(0, original_sentence)\n",
        "  sentence_embeddings = model_ST_bert.encode(asa_set)\n",
        "\n",
        "  cos_sims = cosine_similarity(\n",
        "      [sentence_embeddings[0]],\n",
        "      sentence_embeddings[1:]\n",
        "  )\n",
        "  return cos_sims"
      ],
      "metadata": {
        "id": "WflRoxqpVR4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_important_words_and_sim_scores(original_sentence, combined_set_orig):\n",
        "  z_freqs = {}  \n",
        "  orig_sent_POS_tags = get_pos_tags_list(remove_punct(original_sentence))\n",
        "  POS_tags_dict = {}\n",
        "  for t in orig_sent_POS_tags[0]:\n",
        "    POS_tags_dict[t[0]] = t[1]\n",
        "\n",
        "  original_sentence_arr = remove_punct(original_sentence).split()\n",
        "  for x in original_sentence_arr:\n",
        "    if x not in STOP_WORDS and x not in stop_words_nltk:\n",
        "      print(x.lower(), zipf_frequency(x, 'en'))\n",
        "      z_freqs[x.lower()] = zipf_frequency(x, 'en')\n",
        "    ### some correction to keep possessive pronouns, \n",
        "    if POS_tags_dict[x] == 'PRP$':\n",
        "      if x.lower() not in z_freqs:\n",
        "        print(x, zipf_frequency(x, 'en'))\n",
        "        z_freqs[x.lower()] = zipf_frequency(x, 'en')\n",
        "\n",
        "  z_freqs = sort_dict(z_freqs)\n",
        "\n",
        "  combined_set_copy = list(combined_set_orig).copy()\n",
        "  combined_set_copy.insert(0, original_sentence)\n",
        "  bert_sentence_embeddings = model_ST_bert.encode(combined_set_copy)\n",
        "\n",
        "  cos_sims = cosine_similarity(\n",
        "      [bert_sentence_embeddings[0]],\n",
        "      bert_sentence_embeddings[1:]\n",
        "  )\n",
        "\n",
        "  cos_sim_list = cos_sims[0]\n",
        "  sim_dict = {} # map each unaltered sentence to cosine similarity vs the original sentence\n",
        "  print(\"original sentence: \", original_sentence)\n",
        "  i = 0\n",
        "  sim_col_for_df = []\n",
        "  for x in list(combined_set_orig).copy():\n",
        "    sim_dict[x] = cos_sim_list[i]\n",
        "    sim_col_for_df.append(cos_sim_list[i])\n",
        "    i = i + 1\n",
        "\n",
        "  sim_dict = sort_dict(sim_dict) \n",
        "  print(\"  \")\n",
        "  print(\"Sorted dict:\")\n",
        "  list_of_sent_and_sims = []\n",
        "  for h in list(sim_dict.keys()):\n",
        "    list_of_sent_and_sims.append([h, sim_dict[h]])\n",
        "  list_of_sent_and_sims.reverse()   # get the similarities rankings from top down (high to low)\n",
        "\n",
        "  print(list(z_freqs.keys()))\n",
        "\n",
        "  return list_of_sent_and_sims, z_freqs, sim_col_for_df\n"
      ],
      "metadata": {
        "id": "qsGwXqMQZfGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## get paraphrased sentences \n",
        "## remove sentences that have retained all the original context w/ added extra words\n",
        "## look at the sentences that have substituted one or more of the original context words\n",
        "\n",
        "def get_high_level_sentence_versions(original_sentence, list_of_sent_and_sims, orig_sent_split_arr_lowercase, z_freqs):\n",
        "  orig_sent_word_counts = get_word_counts(original_sentence) \n",
        "  synonyms_dict = {}\n",
        "  filtered_set_soft = []\n",
        "\n",
        "  # print(\"orig_sent_split_arr_lowercase: \", orig_sent_split_arr_lowercase)\n",
        "  # print(\"  \")\n",
        "  test_array = [[\"David likes to eat peanut butter with a spoon in his mouth\", 0]]\n",
        "  skipover = 0\n",
        "  # list_of_sent_and_sims = test_array  ### TEST code\n",
        "  for curr_sent_tup in list_of_sent_and_sims:\n",
        "    # print(\"current iteration: \", curr_sent_tup[0])\n",
        "    curr_sent = curr_sent_tup[0]\n",
        "    orig_sent_arr_copy = orig_sent_split_arr_lowercase.copy()\n",
        "\n",
        "    ### check if the curr sentence should be kept\n",
        "    curr_sent_no_punct_all_lower_split_arr = remove_punct(curr_sent).lower().split()\n",
        "    # word count\n",
        "    curr_sent_word_counts = get_word_counts(curr_sent)\n",
        "    #print(curr_sent_word_counts)\n",
        "    skipover = 0\n",
        "\n",
        "    ### if there is some word that was duplicated, skipover it\n",
        "    #print(\"curr_sent_word_counts.keys(): \", curr_sent_word_counts.keys())\n",
        "    for word in list(orig_sent_word_counts.keys()):\n",
        "      if word not in STOP_WORDS:\n",
        "        if curr_sent_word_counts[word] > orig_sent_word_counts[word]:\n",
        "          skipover = 1\n",
        "    \n",
        "    # print(\"skipover: \", skipover)\n",
        "    ### check if all imporant words are in both\n",
        "    all_wds_in_both = 1\n",
        "    for word in list(z_freqs.keys()):\n",
        "      # print(\"word to check in both: \", word)\n",
        "      # print(\"word in current?: \", word in curr_sent_no_punct_all_lower_split_arr)\n",
        "      if word not in curr_sent_no_punct_all_lower_split_arr:\n",
        "          all_wds_in_both = 0\n",
        "\n",
        "    ## if all important words are in both sentences\n",
        "    # check if there are any added words in the new sentence\n",
        "    # print(\"all_wds_in_both: \", all_wds_in_both)\n",
        "    if all_wds_in_both == 1:\n",
        "      for x in curr_sent_no_punct_all_lower_split_arr:\n",
        "        if x not in list(z_freqs.keys()) and x not in orig_sent_arr_copy :\n",
        "          skipover = 1\n",
        "\n",
        "    if skipover == 0:\n",
        "      filtered_set_soft.append(curr_sent)\n",
        "      for word in orig_sent_arr_copy: \n",
        "        if word in curr_sent_no_punct_all_lower_split_arr:\n",
        "          curr_sent_no_punct_all_lower_split_arr.remove(word)\n",
        "          orig_sent_arr_copy.remove(word)\n",
        "    # print(\"  \")  \n",
        "  print(\"--\")\n",
        "  final_list = []\n",
        "  for h in filtered_set_soft:\n",
        "    xs = remove_punct(h)\n",
        "    if xs not in final_list:\n",
        "      final_list.append(xs)\n",
        "\n",
        "  final_list_lowercase = []   # same exact order as final_list\n",
        "  for c in range (0,len(final_list)):\n",
        "    final_list_lowercase.append(final_list[c].lower())\n",
        "\n",
        "  ### showing all sentences that got through the filter\n",
        "  for c in range (0,len(final_list)):\n",
        "    print(final_list[c])\n",
        "\n",
        "  print(len(final_list_lowercase))\n",
        "  ### get all high level arrangements of the sentence\n",
        "  print(\"\\n\\n\")\n",
        "  orig_sent_no_punct = remove_punct(original_sentence)\n",
        "  all_sent_arrangements_lower = [orig_sent_no_punct.lower()]\n",
        "  all_sent_arrangements = [orig_sent_no_punct]\n",
        "  for ind in range(0, len(final_list_lowercase)):\n",
        "    curr_sent_arr = final_list_lowercase[ind].split()\n",
        "    all_wds_in_sent = 1\n",
        "    # for wd in orig_sent_split_arr_lowercase:\n",
        "    for wd in list(z_freqs.keys()):\n",
        "      if wd not in curr_sent_arr:\n",
        "        all_wds_in_sent = 0\n",
        "    if all_wds_in_sent == 1:\n",
        "      if final_list_lowercase[ind] not in all_sent_arrangements_lower:\n",
        "        all_sent_arrangements_lower.append(final_list_lowercase[ind])\n",
        "        all_sent_arrangements.append(final_list[ind])\n",
        "\n",
        "  asa_set = all_sent_arrangements.copy()\n",
        "  cos_sims = get_cos_similarities(asa_set, original_sentence)\n",
        "\n",
        "  high_level_arr_and_cos_sims_vs_orig = []\n",
        "  print(\"--- all high level sentence arrangements: ---\")\n",
        "  y = 0\n",
        "  for x in all_sent_arrangements:\n",
        "    print(x, cos_sims[0][y])\n",
        "    high_level_arr_and_cos_sims_vs_orig.append([x, cos_sims[0][y]])\n",
        "    y = y + 1\n",
        "    \n",
        "  return filtered_set_soft, high_level_arr_and_cos_sims_vs_orig, all_sent_arrangements_lower, all_sent_arrangements, final_list\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "npyOYhyJ88nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### find out all the matching terms the inference model uses in substitutions\n",
        "## figure out synsets for the words, return the different level categories of the sentences\n",
        "\n",
        "def match_like_terms_and_set_levels(original_sentence, f_list, zipff_freqs, all_sent_arrangements):\n",
        "\n",
        "  zipf_freqs = zipff_freqs.copy()\n",
        "  high_level_arrangements = all_sent_arrangements.copy()\n",
        "  print(\"\\n Showing the sentences in order of cos sim for complexity levels:\")\n",
        "  # final_list still has all the punctuation and normal case\n",
        "  print(\"Original sentence: \", original_sentence)\n",
        "\n",
        "  # dictionary of lists: each word maps to a list of terms, with that term's cos sim and zipf\n",
        "        # zipf of the most important context word if the term has multiple words\n",
        "  syn_set_with_zipf_sim = {}\n",
        "\n",
        "  SELECT_PRNNS = [\"i\", \"my\", \"I\", \"we\", \"our\"]\n",
        "  ### making sure we keep important pronouns in\n",
        "  crucial_context_words = []\n",
        "  for x in list(zipf_freqs.keys()):\n",
        "    pos = get_pos_tags_list(x)\n",
        "    pos = pos[0]\n",
        "    pos_tag = pos[0][1]\n",
        "    if x in SELECT_PRNNS:\n",
        "      crucial_context_words.append(x)\n",
        "  for x in remove_punct(original_sentence).split():\n",
        "    pos = get_pos_tags_list(x)\n",
        "    pos = pos[0]\n",
        "    pos_tag = pos[0][1]\n",
        "    if pos_tag == 'NNP':\n",
        "      crucial_context_words.append(x)\n",
        "\n",
        "  ### remove the top level arrangements from the final_list\n",
        "  final_list = f_list.copy()\n",
        "  for h in high_level_arrangements:\n",
        "    final_list.remove(h)\n",
        "\n",
        "  ### also remove sentences that lost the pronouns\n",
        "  ttt = []\n",
        "  for x in final_list:\n",
        "    remove_sent = 0\n",
        "    # r = remove_punct(x).lower().split() # the correct version, but use later\n",
        "    # for j in crucial_context_words:\n",
        "    #   if j not in r:\n",
        "    for j in crucial_context_words:\n",
        "      if j not in x:\n",
        "        remove_sent = 1\n",
        "    if remove_sent == 1:\n",
        "      ttt.append(x)\n",
        "  for x in ttt:\n",
        "    final_list.remove(x)\n",
        "\n",
        "  ff_list = final_list.copy()\n",
        "  cos_sims_final_list = get_cos_similarities(ff_list, original_sentence)\n",
        "  cos_sims_final_list = cos_sims_final_list[0]  ## should be same length as final_list\n",
        "  \n",
        "  sent_to_sim_dict = {}\n",
        "  print(len(final_list))\n",
        "  #  \n",
        "  for x in range(0, len(final_list)):\n",
        "    sent_to_sim_dict[final_list[x]] = cos_sims_final_list[x]\n",
        "    print(final_list[x], \" || sim_score: \", cos_sims_final_list[x])\n",
        "  \n",
        "  #########################################\n",
        "  #### NOW TO DO THE SYN SET CREATION  ####\n",
        "  #########################################\n",
        "\n",
        "  ### prepping for term matching, list of lists\n",
        "  top_level_no_punct_all_lower_split_arrs = []\n",
        "  for g in high_level_arrangements:\n",
        "    q = remove_punct(g).lower().split()\n",
        "    top_level_no_punct_all_lower_split_arrs.append(q)\n",
        "  \n",
        "  ### first, we need to find all the like terms and put them in the dict of lists\n",
        "  ### next, we find the cut off points for our sentence levels\n",
        "  ### iterate through the final_list until you find the first sentence\n",
        "  ## that has completely removed one context word/phrase from the original\n",
        "  ### The next level down will be filled with those sentences that have only subs\n",
        "  ### and so on\n",
        "  \n",
        "  ### decide on either soft constraints or hard_constraints\n",
        "  ### These 2 are to hold the more complex words that might appear (and have no synonyms created)\n",
        "  soft_constraint_context_set, hard_constraint_context_set = get_important_word_stats(original_sentence, zipf_freqs)\n",
        "  \n",
        "  all_crucial_context_words_soft = crucial_context_words + soft_constraint_context_set\n",
        "  all_crucial_context_words_hard = crucial_context_words + hard_constraint_context_set\n",
        "\n",
        "  ### for each sentence\n",
        "  # the idea is to iteratively remove all the shared context words from both the original \n",
        "  ## and the current sentence, and see if there are any words left that could be like terms\n",
        "  ## this is done by removing shared terms from both top level and current sentence\n",
        "  ## max of 2 context words can remain on the top level\n",
        "  ## then lump up the remaining words in the bottom, doing token splits if needed, and match them\n",
        "  ## against the context words in the top level\n",
        "  ## key note: remember to split the sentence at the determiners to avoid clumping,\n",
        "  ##        since those are the ones most likely to be lost in paraphrasing \n",
        "  ###       trick: if you have a shared determiner, replace it with qqzqq in the current sentence\n",
        "  ###       then later when you do .split(), remember to separate the sentence at the qqzqq\n",
        "\n",
        "  ## the actual sorting of sentences into their respective levels\n",
        "  sents_top_level = high_level_arrangements # no context lost, no substitutions\n",
        "  sents_one_lvl_down = [] # sentences with one or two substitutions\n",
        "  sents_two_lvls_down = [] # subsitutions and losing ONE context word/phrases\n",
        "  sents_three_lvls_down = [] # the rest (either too many subs or too much lost context)\n",
        "  max_zipf_val = np.max(list(zipf_freqs.values()))\n",
        "\n",
        "  ### crude dict of which determiners are attached to which context words:\n",
        "  dets_per_context_word = {}\n",
        "  orig_temp = remove_punct(original_sentence).lower().split()\n",
        "  for i in range(0, len(orig_temp)-1):\n",
        "    if orig_temp[i] in MAIN_DETS:\n",
        "      if orig_temp[i+1] in list(zipf_freqs.keys()):\n",
        "        if orig_temp[i+1] not in dets_per_context_word:\n",
        "          dets_per_context_word[i+1] = [orig_temp[i]]\n",
        "        else:\n",
        "          arr = dets_per_context_word[i+1]\n",
        "          dets_per_context_word[i+1] = arr.append(orig_temp[i])\n",
        "\n",
        "  SYNSET_OF_CONTEXT_WORDS = {}\n",
        "  stop_wd_subset = [\"a\", \"an\", \"the\", \"and\", \"that\", \"it\", \"for\", \"or\", \"but\", \"in\", \"of\",\"not\"]\n",
        "  SPLIT_TOKEN = \"qqzqq\"\n",
        "  KEEP = \"qqqzzqqq\"\n",
        "\n",
        "  print(\"crucial_context_words: \", crucial_context_words)\n",
        "\n",
        "  tlnpalsarrs = top_level_no_punct_all_lower_split_arrs.copy()\n",
        "  sent_top = tlnpalsarrs[0]\n",
        "  tlnp = []\n",
        "  for n in range(0, len(final_list)):\n",
        "    tlnp.append(sent_top.copy())\n",
        "  # final_list = [\"david likes to eat a spoonful of peanut butter with a spoonful\"] # test code\n",
        "  for i in range(0, len(final_list)):\n",
        "    sent = final_list[i]\n",
        "    ### split the sentence into a word array\n",
        "    curr_sent_chunks = []  \n",
        "    curr_sent_chunks.append(remove_punct(sent).lower().split())  \n",
        "\n",
        "    ## top level \n",
        "    \n",
        "\n",
        "    ### first, check if there are any crucial context words missing:\n",
        "    has_all_crucial_context = 1\n",
        "    for x in crucial_context_words:\n",
        "      if x.lower() not in curr_sent_chunks[0]:\n",
        "        has_all_crucial_context = 0\n",
        "    # print(\"iteration: {}\".format(i))\n",
        "    # print(\"this sentence: \", sent)\n",
        "    # print(\"has_all_crucial_context: \", has_all_crucial_context)\n",
        "    if has_all_crucial_context == 0:\n",
        "      continue\n",
        "\n",
        "    sent_one = tlnp[i]\n",
        "    # print(\"sentence top level: \", sent_one)\n",
        "    # print(\"current sentence: \", curr_sent_chunks[0])\n",
        "    chunk = curr_sent_chunks[0]\n",
        "    sent_one_copy = sent_one.copy()\n",
        "    for wd in sent_one_copy:\n",
        "      # print(\"is context word [\", wd, \"] in current sentence: \", wd in chunk)\n",
        "      if wd in chunk:\n",
        "        chunk.remove(wd)\n",
        "        sent_one.remove(wd)\n",
        "    #     print(\"top level sentence after removal: \", sent_one)\n",
        "    #     print(\"current sentence after removal: \", chunk)\n",
        "    # print(\"\\n what's left:\")\n",
        "    # print(\"top level sentence afer matching: \", sent_one)\n",
        "    # print(\"current sentence after matching: \", chunk)\n",
        "    remaining_words = chunk\n",
        "    # print(SYNSET_OF_CONTEXT_WORDS)\n",
        "    ### now we check the remaining words left in current sentence\n",
        "    ###splitting process, hopefully we will have synonym sets already found by the inference model,\n",
        "    ### else, we just leave it in the lowest level group\n",
        "    ### if the bottom level is empty/only useless words, either all matched or missing context\n",
        "    result = []\n",
        "    if len(remaining_words) > 0:\n",
        "      ### make sure it's not just only stop words left in the bottom level before combining the word\n",
        "      all_stop_wds = 1\n",
        "      for wd in remaining_words:\n",
        "        if wd not in STOP_WORDS:\n",
        "          all_stop_wds = 0\n",
        "      if all_stop_wds == 0:\n",
        "        str_together = remaining_words[0]\n",
        "        i = 1\n",
        "        while i < len(remaining_words):\n",
        "          str_together = str_together + \" \" + remaining_words[i]\n",
        "          i = i + 1\n",
        "      else:\n",
        "        str_together = \"\"\n",
        "    else:\n",
        "      str_together = \"\"\n",
        "    \n",
        "    for x in sent_one:\n",
        "      if x in STOP_WORDS:\n",
        "        sent_one.remove(x)\n",
        "    \n",
        "    # nothing to check, maybe some minor change to the original\n",
        "    if len(sent_one) == 0:\n",
        "      sents_one_lvl_down.append(sent)\n",
        "\n",
        "    # only one sub or one context lost\n",
        "    elif len(sent_one) == 1:  \n",
        "      if str_together == \"\":\n",
        "        sents_two_lvls_down.append(sent)\n",
        "      else:\n",
        "        this_word = sent_one[0]\n",
        "        # print(\"this_word: \", this_word)\n",
        "        # print(\"curr sent left: \", str_together)\n",
        "        # bb = this_word not in list(SYNSET_OF_CONTEXT_WORDS.keys())\n",
        "        # print(\"this_word not in list(SYNSET_OF_CONTEXT_WORDS.keys(): \", bb)\n",
        "        # print(\"SYNSET_OF_CONTEXT_WORDS before operations: \", SYNSET_OF_CONTEXT_WORDS)\n",
        "        if this_word not in list(SYNSET_OF_CONTEXT_WORDS.keys()):\n",
        "          SYNSET_OF_CONTEXT_WORDS[this_word] = [str_together]\n",
        "        else:\n",
        "          # temp = SYNSET_OF_CONTEXT_WORDS[this_word].copy()\n",
        "          SYNSET_OF_CONTEXT_WORDS[this_word].append(str_together)\n",
        "        sents_one_lvl_down.append(sent)\n",
        "        # print(\"SYNSET_OF_CONTEXT_WORDS AFTER operations: \", SYNSET_OF_CONTEXT_WORDS)\n",
        "    ## three cases\n",
        "    elif len(sent_one) == 2:   \n",
        "      if str_together == \"\":  ## zero syns in the bottom case\n",
        "        sents_three_lvls_down.append(sent)\n",
        "      else: ## one or two syns in the bottom\n",
        "        for con_word in sent_one:  \n",
        "          # remove synonyms we already before\n",
        "          if con_word in list(SYNSET_OF_CONTEXT_WORDS.keys()):\n",
        "            for possible_syn in SYNSET_OF_CONTEXT_WORDS[con_word]:\n",
        "              if possible_syn in str_together:\n",
        "                str_together = str_together.replace(str_together, \"\")\n",
        "                str_together = str_together.replace(\"  \", \" \")\n",
        "                str_together = str_together.strip()\n",
        "                sent_one.remove(con_word)  ## removed the word from top level, and syn from bottom level\n",
        "        \n",
        "        ### Hardest case:\n",
        "        ### if we have 2 context words left somehow (unlikely), and unknown amount of tokens in str_together\n",
        "        ## try multiple splits on str_together, run them through the similarity checker to find the \n",
        "        ### best split on the remaning curr sentence words for both context words\n",
        "        if len(sent_one) == 2:\n",
        "          asdf =7\n",
        "        if len(sent_one) == 1:  ## was able to remove a pre-existing synonym/similar phrase\n",
        "          if str_together == \"\": ## one context lost\n",
        "            sents_two_lvls_down.append(sent)\n",
        "          else: ## no context lost, new syn found\n",
        "            this_word = sent_one[0]\n",
        "            if this_word not in list(SYNSET_OF_CONTEXT_WORDS.keys()):\n",
        "              SYNSET_OF_CONTEXT_WORDS[this_word] = [str_together]\n",
        "            else:\n",
        "              temp = SYNSET_OF_CONTEXT_WORDS[this_word].copy()\n",
        "              SYNSET_OF_CONTEXT_WORDS[this_word] = temp.append(str_together)\n",
        "            sents_one_lvl_down.append(sent)\n",
        "\n",
        "    ## too much still remaining in top level, either too many subs had to happen, or too much context lost\n",
        "    else:  \n",
        "      sents_three_lvls_down.append(sent)\n",
        "\n",
        "    ## after all common words removed from both current and top level\n",
        "    ### check how many important context words are lost in the current sentence\n",
        "    # this is done by checking the top level ones:\n",
        "    # if there is 1 context word left in the top level arrangements, and nothing but uesless words\n",
        "    ## in the current sentence, then 1 context word was lost\n",
        "    ## if you can match all the tokens in the current sentence, then no context was lost\n",
        "\n",
        "  return sents_top_level, sents_one_lvl_down, sents_two_lvls_down, sents_three_lvls_down, SYNSET_OF_CONTEXT_WORDS\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "_aM83ZzPR5Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify_sentence(original_sentence):\n",
        "  orig_sentence_no_punct = remove_punct(original_sentence)\n",
        "  orig_sentence_no_punct_all_lower = orig_sentence_no_punct.lower()\n",
        "\n",
        "  orig_sent_no_punct_split_arr = orig_sentence_no_punct.split()\n",
        "  orig_sent_no_punct_all_lower_split_arr = orig_sentence_no_punct_all_lower.split()\n",
        "\n",
        "  final_sent_orig = orig_sentence_no_punct  #original sentence with no punctuation \n",
        "\n",
        "  set_1 = set(get_paraphrased_sentences(model_pega, tokenizer_pega, original_sentence, num_beams=20, num_return_sequences=20))\n",
        "  set_2 = set(get_paraphrased_sentences(model_paws, tokenizer_paws, original_sentence, num_beams=20, num_return_sequences=20))\n",
        "  set_3 = set(get_paraphrased_sentences(model_parr, tokenizer_parr, original_sentence, num_beams=20, num_return_sequences=20))\n",
        "  set_x = set(get_paraphrased_sentences(model_ctrl44, tokenizer_ctrl44, original_sentence, num_beams=20, num_return_sequences=20))\n",
        "  set_xy = set(get_paraphrased_sentences(model_ctrl44, tokenizer_ctrl44, \"<para> \"+original_sentence, num_beams=20, num_return_sequences=20))\n",
        "\n",
        "  id_tags = []\n",
        "  k = 0\n",
        "  full_set_orig = []  # with punctuation all normal case\n",
        "  full_set_no_punct = []  # with no punctuation, all normal case\n",
        "  full_set_no_punct_all_lower = [] # with no punctuation, all lower case\n",
        "\n",
        "  combined_set_orig = set_1.union(set_2).union(set_3).union(set_x).union(set_xy)\n",
        "  # print(\"all possible sentenecs: \")\n",
        "  for f in combined_set_orig:\n",
        "    # print(f)\n",
        "    full_set_orig.append(f)\n",
        "    full_set_no_punct.append(remove_punct(f))\n",
        "    full_set_no_punct_all_lower.append(remove_punct(f).lower())\n",
        "    id_tags.append(k)\n",
        "    k = k + 1\n",
        "  print(\" \")\n",
        "\n",
        "  d = {'SentID': id_tags, 'OrigSent': full_set_orig, 'NoPunctSent': full_set_no_punct, 'NoPunctAllLower': full_set_no_punct_all_lower}\n",
        "  sentences_df = pd.DataFrame(data=d)\n",
        "  full_sentence_df = sentences_df.copy(deep=True)  # holds all the sentences and tags\n",
        "  print(\"number of sentences\", len(full_sentence_df))\n",
        "  \n",
        "  list_of_sent_and_sims, z_freqs, sim_col_for_df = get_important_words_and_sim_scores(original_sentence, combined_set_orig)\n",
        "  full_sentence_df['Cos_Sim_vs_Orig_Sent'] = sim_col_for_df\n",
        "  filtered_set_soft, high_level_arr_and_cos_sims_vs_orig, all_sent_arrangements_lower, all_sent_arrangements, final_list = get_high_level_sentence_versions(original_sentence, list_of_sent_and_sims, orig_sent_no_punct_all_lower_split_arr, z_freqs)\n",
        "  sents_top_level, sents_one_lvl_down, sents_two_lvls_down, sents_three_lvls_down, syndict = match_like_terms_and_set_levels(original_sentence, final_list, z_freqs, all_sent_arrangements)\n",
        "  \n",
        "  print(\"   \\n\\n\\n\")\n",
        "  print(\"Results:\")\n",
        "  print(\"Level 0: \")\n",
        "  for x in sents_top_level:\n",
        "    print(x)\n",
        "  print(\"\\n\")\n",
        "  print(\"Level -1: \")\n",
        "  for x in sents_one_lvl_down:\n",
        "    print(x)\n",
        "  print(\"\\n\")\n",
        "  print(\"Level -2: \")\n",
        "  for x in sents_two_lvls_down:\n",
        "    print(x)\n",
        "  print(\"\\n\")\n",
        "  print(\"Level -3: \")\n",
        "  for x in sents_three_lvls_down:\n",
        "    print(x)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  PREPS_COMMON = [\"with\", \"at\", \"by\", \"to\", \"in\", \"for\", \"from\", \"of\", \"on\"]\n",
        "\n",
        "  print(\"Synsets Breakdown: \")\n",
        "  for k in list(syndict.keys()):\n",
        "    print(\"Word: \", k)\n",
        "    print(\"----------------------------\")\n",
        "    t = list(set(syndict[k]))\n",
        "    syns_in_order_of_sim = {}\n",
        "    syns_listing = []\n",
        "    for y in t:\n",
        "      if len(y.split()) == 2:\n",
        "        syn_split = y.split()\n",
        "        pos_list = get_pos_tags_list(y)[0]\n",
        "        first_term = pos_list[0][1]\n",
        "        second_term = pos_list[1][1]\n",
        "        if first_term in list(MAIN_POS_TAGS.keys()) and second_term in list(MAIN_POS_TAGS.keys()):\n",
        "          if MAIN_POS_TAGS[second_term] == \"verb\" and MAIN_POS_TAGS[first_term] != 'adverb':\n",
        "            temp_list = []\n",
        "            for ws in PREPS_COMMON:\n",
        "              temp_list.append(syn_split[0] + \" \" + ws + \" \" + syn_split[1])\n",
        "            temp_temp_list = temp_list.copy()\n",
        "            cos_sims_preps_added = get_cos_similarities(temp_temp_list, k)\n",
        "            cos_sims_preps_added = cos_sims_preps_added[0]\n",
        "            phrase_dict_sims = {}\n",
        "            for ind in range(0, len(temp_list)):\n",
        "              phrase_dict_sims[temp_list[ind]] = cos_sims_preps_added[ind]\n",
        "            phrase_dict_sims = sort_dict(phrase_dict_sims)\n",
        "            best_key = list(phrase_dict_sims.keys())[0]\n",
        "            syns_listing.append(best_key)\n",
        "      else:\n",
        "        syns_listing.append(y)\n",
        "    temp_syns_listing = syns_listing.copy()\n",
        "    sims_of_syns = get_cos_similarities(temp_syns_listing, k)\n",
        "    sims_of_syns = sims_of_syns[0]\n",
        "    for ind in range(0, len(syns_listing)):\n",
        "      syns_in_order_of_sim[syns_listing[ind]] = sims_of_syns[ind]\n",
        "    syns_in_order_of_sim = sort_dict(syns_in_order_of_sim)\n",
        "    tt = list(syns_in_order_of_sim.keys())\n",
        "    for sind in range(len(tt)-1, -1, -1):\n",
        "      synonym = tt[sind]\n",
        "      print(\"\\t\",synonym)\n",
        "    print(\" \")\n",
        "\n",
        "\n",
        "    ### last step: add syns for words that aren't in spacy medium using nltk SynSets\n"
      ],
      "metadata": {
        "id": "ALS7t-fYVuDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gunning_fog(\"I photographed\"))\n",
        "print(gunning_fog(\"I took a picture\"))\n",
        "print(gunning_fog(\"I took a photo\"))\n",
        "\n",
        "print(\" \")\n",
        "print(zipf_frequency(\"photographed\", 'en'))\n",
        "print(zipf_frequency(\"picture\", 'en'))\n",
        "print(zipf_frequency(\"photo\", 'en'))\n",
        "\n",
        "print(get_cos_similarities([\"I took a picture\", \"I took a photo\"], \"I photographed\"))\n",
        "\n",
        "print(zipf_frequency(\"moisture\", 'en'))\n",
        "\n",
        "\n",
        "print(get_cos_similarities([\"appear from disappear\", \"appear to disappear\", \"appear by disappear\"], \"vanish\"))"
      ],
      "metadata": {
        "id": "twt7RG1_Tnfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(zipf_frequency(\"discretion\", 'en'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imPaCO5FgUQo",
        "outputId": "89e02f42-50ef-4e0a-ef69-94bd6fc63aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_zipfs_in_spacy = []\n",
        "for x in nlp_medium_words:\n",
        "  all_zipfs_in_spacy.append(zipf_frequency(x, 'en'))\n",
        "\n",
        "print(np.mean(all_zipfs_in_spacy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-BHAYQBe2ZZ",
        "outputId": "13144877-0a8f-48c4-e5c9-ad3975a76bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5555920326503698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.quantile(all_zipfs_in_spacy, .957))  ### and this one\n",
        "print(np.quantile(all_zipfs_in_spacy, .90))  ### probably use this one\n",
        "print(np.quantile(all_zipfs_in_spacy, .897))\n",
        "\n",
        "### in between .95 and .9 are harder words that may be known"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdOzaGZMfufw",
        "outputId": "742c91e7-77dc-487c-8b4e-7af211bd5480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.24\n",
            "3.38\n",
            "3.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nlp_medium(\"photograph photo picture image\")\n",
        "  \n",
        "for token in tokens:\n",
        "    # Printing the following attributes of each token.\n",
        "    # text: the word string, has_vector: if it contains\n",
        "    # a vector representation in the model, \n",
        "    # vector_norm: the algebraic norm of the vector,\n",
        "    # is_oov: if the word is out of vocabulary.\n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
        "\n",
        "for t1 in range(0, len(tokens)):\n",
        "  token1 = tokens[t1]\n",
        "  for t2 in range(t1, len(tokens)):\n",
        "    token2 = tokens[t2]\n",
        "    print(\"Similarity: {}, {}\".format(token1, token2), token1.similarity(token2))\n",
        "\n",
        "print(get_cos_similarities([\"photo\", \"picture\", \"image\"], \"photograph\"))\n",
        "\n",
        "\n",
        "get_pos_tags_list(\"Miles ate a hamburger\")"
      ],
      "metadata": {
        "id": "a1weOWavWuEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### if no synonyms generated from the inference models, or if we want another option, switch to Synsets\n",
        "# substitute = get_similar_words(\"prolixity\", \"noun\")\n",
        "sgsg = get_similar_words(\"reticence\", \"noun\")\n",
        "print(\"sub: \", sgsg)"
      ],
      "metadata": {
        "id": "ABUlqDBGy-zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yQQqFxH3i_4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test out Our Program!"
      ],
      "metadata": {
        "id": "H_GBF8FVBzHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = input() \n",
        "simplify_sentence(h)\n",
        "\n"
      ],
      "metadata": {
        "id": "6RZw3SDjB0f0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}